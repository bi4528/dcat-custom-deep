{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80e23c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef, RDFS, RDF, OWL, Literal, Namespace\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d32e3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e621160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/hpc/home/bi4528/ckanenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "450cd5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4467d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOUSE_OWL_PATH = \"./data/datasets/anatomy-dataset/mouse.owl\"\n",
    "HUMAN_OWL_PATH = \"./data/datasets/anatomy-dataset/human.owl\"\n",
    "ALIGNMENT_RDF_PATH = \"./data/datasets/anatomy-dataset/reference.rdf\"\n",
    "HUMAN_NAMESPACE = \"http://human.owl#\"\n",
    "MOUSE_NAMESPACE = \"http://mouse.owl#\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d533de",
   "metadata": {},
   "source": [
    "Getting ontology enriched terms from human.owl to be FAISS friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dff7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "ONTOLOGY_INDEX_PATH = \"./data/models/ontology_index.faiss\"\n",
    "ONTOLOGY_TRACKER_PATH = \"./data/models/ontology_id_tracker.json\"\n",
    "EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b591d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N8ff3d966ec7147be8ce0f2ea00a8b7cc (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mouse_graph = Graph()\n",
    "mouse_graph.parse(MOUSE_OWL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b776c1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N72502864318f4c94904b45445d0fcae0 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_graph = Graph()\n",
    "human_graph.parse(HUMAN_OWL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74faa4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N393057ed888f4e838964e48977b6da3f (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignment_graph = Graph()\n",
    "alignment_graph.parse(ALIGNMENT_RDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f92941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_for_embedding(label, definition=None, synonyms=None, superclasses=None):\n",
    "    parts = [f\"Concept: {label}\"]\n",
    "\n",
    "    if synonyms:\n",
    "        parts.append(f\"Also known as: {', '.join(synonyms)}\")\n",
    "\n",
    "    if superclasses:\n",
    "        parts.append(f\"Part of: {', '.join(superclasses)}\")\n",
    "\n",
    "    if definition:\n",
    "        parts.append(f\"Defined as: {definition}\")\n",
    "\n",
    "    return \". \".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cce60a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(graph, uri):\n",
    "    label = graph.value(uri, RDFS.label)\n",
    "    return str(label) if isinstance(label, Literal) else None\n",
    "\n",
    "def extract_related_uris(graph, subject, predicate):\n",
    "    \"\"\"Dereferences URIs linked by the predicate and returns their rdfs:label.\"\"\"\n",
    "    values = []\n",
    "    for obj in graph.objects(subject, predicate):\n",
    "        label = get_label(graph, obj)\n",
    "        if label:\n",
    "            values.append(label)\n",
    "    return values\n",
    "\n",
    "def extract_superclass_labels(graph, subject):\n",
    "    \"\"\"Get human-readable labels of direct superclasses.\"\"\"\n",
    "    super_labels = []\n",
    "    for superclass in graph.objects(subject, RDFS.subClassOf):\n",
    "        if isinstance(superclass, URIRef):\n",
    "            label = get_label(graph, superclass)\n",
    "            if label:\n",
    "                super_labels.append(label)\n",
    "        elif (superclass, RDF.type, OWL.Restriction) in graph:\n",
    "            filler = graph.value(superclass, OWL.someValuesFrom)\n",
    "            if isinstance(filler, URIRef):\n",
    "                super_labels.append(str(filler).split(\"#\")[-1])\n",
    "    return super_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48ede41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/ontology/ontology_terms_enriched.json', 3298)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OBO = Namespace(\"http://www.geneontology.org/formats/oboInOwl#\")\n",
    "\n",
    "# Now re-run the enrichment step\n",
    "terms = []\n",
    "for s in human_graph.subjects(RDF.type, OWL.Class):\n",
    "    label = get_label(human_graph, s)\n",
    "    if not label:\n",
    "        continue\n",
    "\n",
    "    definition = extract_related_uris(human_graph, s, OBO.hasDefinition)\n",
    "    synonyms = extract_related_uris(human_graph, s, OBO.hasRelatedSynonym)\n",
    "    superclasses = extract_superclass_labels(human_graph, s)\n",
    "\n",
    "    text_parts = [label]\n",
    "    if definition:\n",
    "        text_parts.append(definition[0])\n",
    "    if synonyms:\n",
    "        text_parts.append(\"Synonyms: \" + \"; \".join(synonyms))\n",
    "    if superclasses:\n",
    "        text_parts.append(\"Superclass: \" + \"; \".join(superclasses))\n",
    "\n",
    "    enriched_text = build_text_for_embedding(\n",
    "        label=label,\n",
    "        definition=definition[0] if definition else None,\n",
    "        synonyms=synonyms,\n",
    "        superclasses=superclasses\n",
    "    )\n",
    "\n",
    "    terms.append({\n",
    "        \"uri\": str(s),\n",
    "        \"label\": label,\n",
    "        \"definition\": definition[0] if definition else \"\",\n",
    "        \"synonyms\": synonyms,\n",
    "        \"superclasses\": superclasses,\n",
    "        \"text_for_embedding\": enriched_text\n",
    "    })\n",
    "\n",
    "# Save enriched ontology terms to JSON\n",
    "output_path = \"./data/ontology/ontology_terms_enriched.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(terms, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "output_path, len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f50c6d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/ontology/mouse_terms_enriched.json', 2737)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OBO = Namespace(\"http://www.geneontology.org/formats/oboInOwl#\")\n",
    "\n",
    "# Now re-run the enrichment step\n",
    "terms = []\n",
    "for s in mouse_graph.subjects(RDF.type, OWL.Class):\n",
    "    label = get_label(mouse_graph, s)\n",
    "    if not label:\n",
    "        continue\n",
    "\n",
    "    definition = extract_related_uris(mouse_graph, s, OBO.hasDefinition)\n",
    "    synonyms = extract_related_uris(mouse_graph, s, OBO.hasRelatedSynonym)\n",
    "    superclasses = extract_superclass_labels(mouse_graph, s)\n",
    "\n",
    "    text_parts = [label]\n",
    "    if definition:\n",
    "        text_parts.append(definition[0])\n",
    "    if synonyms:\n",
    "        text_parts.append(\"Synonyms: \" + \"; \".join(synonyms))\n",
    "    if superclasses:\n",
    "        text_parts.append(\"Superclass: \" + \"; \".join(superclasses))\n",
    "\n",
    "    enriched_text = build_text_for_embedding(\n",
    "        label=label,\n",
    "        definition=definition[0] if definition else None,\n",
    "        synonyms=synonyms,\n",
    "        superclasses=superclasses\n",
    "    )\n",
    "\n",
    "    terms.append({\n",
    "        \"uri\": str(s),\n",
    "        \"label\": label,\n",
    "        \"definition\": definition[0] if definition else \"\",\n",
    "        \"synonyms\": synonyms,\n",
    "        \"superclasses\": superclasses,\n",
    "        \"text_for_embedding\": enriched_text\n",
    "    })\n",
    "\n",
    "# Save enriched ontology terms to JSON\n",
    "output_path = \"./data/ontology/mouse_terms_enriched.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(terms, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "output_path, len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c422fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namespaces\n",
    "ALIGN = Namespace(\"http://knowledgeweb.semanticweb.org/heterogeneity/alignment\")\n",
    "\n",
    "# Step 1: Extract gold URI mappings from reference.rdf\n",
    "gold_mappings = {}\n",
    "for cell in alignment_graph.subjects(RDF.type, ALIGN.Cell):\n",
    "    mouse_uri = alignment_graph.value(cell, ALIGN.entity1)\n",
    "    human_uri = alignment_graph.value(cell, ALIGN.entity2)\n",
    "    if isinstance(mouse_uri, URIRef) and isinstance(human_uri, URIRef):\n",
    "        gold_mappings[str(mouse_uri)] = str(human_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b373fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = []\n",
    "for s in mouse_graph.subjects(RDF.type, OWL.Class):\n",
    "    label = mouse_graph.value(s, RDFS.label)\n",
    "    comment = mouse_graph.value(s, RDFS.comment)\n",
    "    if label:\n",
    "        entry = {\n",
    "            \"uri\": str(s),\n",
    "            \"label\": str(label),\n",
    "            \"description\": str(comment) if isinstance(comment, Literal) else \"\",\n",
    "            \"gold_uri\": gold_mappings.get(str(s), \"\")\n",
    "        }\n",
    "        if entry[\"gold_uri\"]:\n",
    "            testset.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e14ac092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/datasets/anatomy-dataset/mouse_testset.json', 1497)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_path = \"./data/datasets/anatomy-dataset/mouse_testset.json\"\n",
    "with open(testset_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(testset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "testset_path, len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a32c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./data/ontology/ontology_terms_enriched.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    human_terms = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07455831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95fda8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d768802",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONTOLOGY_JSON = \"./data/ontology/ontology_terms_enriched.json\"\n",
    "FAISS_INDEX_PATH = \"./data/models/ontology_index.faiss\"\n",
    "MOUSE_FAISS_INDEX_PATH = \"./data/models/mouse_ontology_index.faiss\"\n",
    "ID_TRACKER_JSON = \"./data/models/ontology_id_tracker.json\"\n",
    "MOUSE_ID_TRACKER_JSON = \"./data/models/mouse_ontology_id_tracker.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93e667bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d1eef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ontology_indexing(ontology_json=ONTOLOGY_JSON, faiss_index_path=FAISS_INDEX_PATH, id_tracker_json=ID_TRACKER_JSON):\n",
    "\n",
    "    with open(ontology_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            ontology_terms = json.load(f)\n",
    "\n",
    "    texts = []\n",
    "    ids = []\n",
    "    valid_terms = []\n",
    "\n",
    "    for i, term in enumerate(ontology_terms):\n",
    "        text = term.get(\"text_for_embedding\")\n",
    "        if not text:\n",
    "            raise ValueError(\"Missing 'text_for_embedding'\")\n",
    "        texts.append(text)\n",
    "        ids.append(abs(hash(term[\"uri\"])) % (10**12))\n",
    "        valid_terms.append(term)\n",
    "\n",
    "    # Embedding\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    embeddings = model.encode(texts, batch_size=16, show_progress_bar=True)\n",
    "    embeddings = normalize_embeddings(np.array(embeddings))\n",
    "\n",
    "    # FAISS indexing\n",
    "    dimension = embeddings.shape[1]\n",
    "    base_index = faiss.IndexFlatIP(dimension)\n",
    "    index = faiss.IndexIDMap(base_index)\n",
    "    index.add_with_ids(embeddings, np.array(ids))\n",
    "    os.makedirs(os.path.dirname(faiss_index_path), exist_ok=True)\n",
    "    faiss.write_index(index, faiss_index_path)\n",
    "\n",
    "    # Save ID tracker\n",
    "    id_map = {str(id_): term for id_, term in zip(ids, valid_terms)}\n",
    "    with open(id_tracker_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(id_map, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e53ca08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 207/207 [00:14<00:00, 13.97it/s]\n"
     ]
    }
   ],
   "source": [
    "ontology_indexing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eac64fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model + index\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=\"cpu\")\n",
    "ontology_index = faiss.read_index(ONTOLOGY_INDEX_PATH)\n",
    "with open(ONTOLOGY_TRACKER_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    ontology_id_map = {int(k): v for k, v in json.load(f).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e1ab503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms\n",
    "\n",
    "def match_to_ontology(text, top_k=1):\n",
    "    embedding = embedding_model.encode([text])\n",
    "    embedding = normalize_embeddings(np.array(embedding)).astype(np.float32)\n",
    "    D, I = ontology_index.search(embedding, top_k)\n",
    "    matches = []\n",
    "    for idx, score in zip(I[0], D[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        term = ontology_id_map.get(idx, {})\n",
    "        matches.append({\n",
    "            \"uri\": term.get(\"uri\"),\n",
    "            \"label\": term.get(\"label\"),\n",
    "            \"score\": float(score),\n",
    "            \"description\": term.get(\"description\")\n",
    "        })\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb37e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test examples: 1497\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/datasets/anatomy-dataset/mouse_testset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    testset = json.load(f)\n",
    "\n",
    "print(\"Total test examples:\", len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ae5dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-execute helper functions after kernel reset\n",
    "\n",
    "def normalize_label(label):\n",
    "    \"\"\"Lowercase and remove non-alphanumeric characters.\"\"\"\n",
    "    return re.sub(r'[^a-z0-9 ]+', '', label.lower().strip())\n",
    "\n",
    "def build_lexical_index(human_terms, top_k=50):\n",
    "    \"\"\"\n",
    "    Builds a TF-IDF index over human labels and prepares for quick filtering.\n",
    "\n",
    "    Args:\n",
    "        human_terms (List[Dict]): Each term should have a 'label' key.\n",
    "        top_k (int): Number of top lexical candidates to return.\n",
    "\n",
    "    Returns:\n",
    "        lexical_filter_fn: A function that returns top_k lexical matches for a given mouse label.\n",
    "    \"\"\"\n",
    "    labels = [normalize_label(t[\"label\"]) for t in human_terms]\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2)).fit(labels)\n",
    "    label_matrix = vectorizer.transform(labels)\n",
    "\n",
    "    def lexical_filter_fn(query_label):\n",
    "        query_vec = vectorizer.transform([normalize_label(query_label)])\n",
    "        sim = cosine_similarity(query_vec, label_matrix).flatten()\n",
    "        top_indices = np.argsort(sim)[-top_k:][::-1]\n",
    "        return [(i, sim[i]) for i in top_indices if sim[i] > 0]\n",
    "\n",
    "    return lexical_filter_fn\n",
    "\n",
    "def run_batch_faiss(query_embeddings, faiss_index, top_k=5):\n",
    "    \"\"\"\n",
    "    Runs batch FAISS search.\n",
    "\n",
    "    Args:\n",
    "        query_embeddings (np.ndarray): Shape (n_queries, dim), should be normalized.\n",
    "        faiss_index: FAISS index of normalized human embeddings.\n",
    "        top_k (int): Number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "        D, I: FAISS distances and indices\n",
    "    \"\"\"\n",
    "    return faiss_index.search(query_embeddings, top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70120d12",
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'matmul' did not contain a loop with signature matching types (dtype('float32'), dtype('<U1121')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUFuncTypeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m query_emb = normalize_embeddings(np.array(query_emb)).astype(np.float32)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 4. FAISS match only against filtered pool (manual since sub-index)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m similarities = (\u001b[43mquery_emb\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltered_embeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m).flatten()\n\u001b[32m     37\u001b[39m top_k = np.argsort(similarities)[-\u001b[32m5\u001b[39m:][::-\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# top-5 by similarity\u001b[39;00m\n\u001b[32m     39\u001b[39m top_matches = [\n\u001b[32m     40\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33muri\u001b[39m\u001b[33m\"\u001b[39m: filtered_uris[i], \u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(similarities[i])}\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_k\n\u001b[32m     42\u001b[39m ]\n",
      "\u001b[31mUFuncTypeError\u001b[39m: ufunc 'matmul' did not contain a loop with signature matching types (dtype('float32'), dtype('<U1121')) -> None"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Prepare: build lexical filter\n",
    "lexical_filter_fn = build_lexical_index(human_terms, top_k=50)\n",
    "\n",
    "# Precompute embeddings for all human terms\n",
    "human_embeddings = np.array([t[\"text_for_embedding\"] for t in human_terms])  # already normalized\n",
    "human_labels = [t[\"label\"] for t in human_terms]\n",
    "human_uris = [t[\"uri\"] for t in human_terms]\n",
    "\n",
    "for case in testset:\n",
    "    query_label = case[\"label\"]\n",
    "    gold = case[\"gold_uri\"]\n",
    "\n",
    "    # 1. Lexical pre-filter\n",
    "    filtered_indices = [idx for idx, _ in lexical_filter_fn(query_label)]\n",
    "    if not filtered_indices:\n",
    "        results.append({\n",
    "            \"query\": query_label,\n",
    "            \"gold_uri\": gold,\n",
    "            \"predicted_uri\": None,\n",
    "            \"confidence\": 0,\n",
    "            \"found_at_rank\": None\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # 2. Get filtered embeddings + metadata\n",
    "    filtered_embeddings = human_embeddings[filtered_indices]\n",
    "    filtered_uris = [human_uris[i] for i in filtered_indices]\n",
    "\n",
    "    # 3. Encode query and normalize\n",
    "    query_emb = embedding_model.encode([query_label])\n",
    "    query_emb = normalize_embeddings(np.array(query_emb)).astype(np.float32)\n",
    "\n",
    "    # 4. FAISS match only against filtered pool (manual since sub-index)\n",
    "    similarities = (query_emb @ filtered_embeddings.T).flatten()\n",
    "    top_k = np.argsort(similarities)[-5:][::-1]  # top-5 by similarity\n",
    "\n",
    "    top_matches = [\n",
    "        {\"uri\": filtered_uris[i], \"score\": float(similarities[i])}\n",
    "        for i in top_k\n",
    "    ]\n",
    "\n",
    "    found_at = next((i for i, match in enumerate(top_matches) if match[\"uri\"] == gold), None)\n",
    "\n",
    "    results.append({\n",
    "        \"query\": query_label,\n",
    "        \"gold_uri\": gold,\n",
    "        \"predicted_uri\": top_matches[0][\"uri\"] if top_matches else None,\n",
    "        \"confidence\": top_matches[0][\"score\"] if top_matches else 0,\n",
    "        \"found_at_rank\": found_at\n",
    "    })\n",
    "\n",
    "# Final metrics\n",
    "at_1 = sum(1 for r in results if r[\"found_at_rank\"] == 0) / len(results)\n",
    "at_5 = sum(1 for r in results if r[\"found_at_rank\"] is not None) / len(results)\n",
    "\n",
    "print(f\"mapping@1: {at_1:.2%}\")\n",
    "print(f\"mapping@5: {at_5:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4452d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "model = SentenceTransformer(EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a5f2e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/ontology/mouse_terms_enriched.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mouse_terms = json.load(f)\n",
    "\n",
    "with open(ID_TRACKER_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    human_map = json.load(f)\n",
    "    human_map = {int(k): v for k, v in human_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae4d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96b57181",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "\n",
    "for term in mouse_terms:\n",
    "    text = term.get(\"text_for_embedding\")\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    emb = model.encode([text])\n",
    "    emb = normalize_embeddings(np.array(emb).astype(np.float32))\n",
    "\n",
    "    D, I = index.search(emb, 5)\n",
    "\n",
    "    results = []\n",
    "    for idx, score in zip(I[0], D[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        h_term = human_map.get(idx)\n",
    "        results.append({\n",
    "            \"uri\": h_term.get(\"uri\"),\n",
    "            \"label\": h_term.get(\"label\"),\n",
    "            \"score\": float(score)\n",
    "        })\n",
    "\n",
    "    matches.append({\n",
    "        \"mouse_uri\": term[\"uri\"],\n",
    "        \"mouse_label\": term[\"label\"],\n",
    "        \"top_match\": results[0] if results else None,\n",
    "        \"top_k_matches\": results\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8482b5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching complete. Results saved to ./data/datasets/anatomy-dataset/mouse_to_human_matches.json\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./data/datasets/anatomy-dataset/mouse_to_human_matches.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(matches, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Matching complete. Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9812678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz.fuzz import ratio\n",
    "\n",
    "def rerank_by_label_similarity(mouse_label, top_k_matches, weight_faiss=0.7, weight_label=0.3):\n",
    "    \"\"\"\n",
    "    Re-ranks top_k_matches based on a combined score of FAISS embedding similarity and label similarity.\n",
    "\n",
    "    Each match must contain: label, score\n",
    "    \"\"\"\n",
    "    reranked = []\n",
    "    for match in top_k_matches:\n",
    "        candidate_label = match.get(\"label\", \"\")\n",
    "        label_sim = ratio(mouse_label, candidate_label) / 100  # normalize to [0,1]\n",
    "\n",
    "        combined_score = weight_faiss * match[\"score\"] + weight_label * label_sim\n",
    "\n",
    "        reranked.append({\n",
    "            **match,\n",
    "            \"label_similarity\": label_sim,\n",
    "            \"combined_score\": combined_score\n",
    "        })\n",
    "\n",
    "    return sorted(reranked, key=lambda m: -m[\"combined_score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "536b5ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 172/172 [00:04<00:00, 36.52it/s]\n"
     ]
    }
   ],
   "source": [
    "ontology_indexing(ontology_json=\"./data/ontology/mouse_terms_enriched.json\", faiss_index_path=MOUSE_FAISS_INDEX_PATH, id_tracker_json=MOUSE_ID_TRACKER_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8031c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MOUSE_ID_TRACKER_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    mouse_index = faiss.read_index(MOUSE_FAISS_INDEX_PATH)\n",
    "    mouse_id_map = {int(k): v for k, v in json.load(f).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d54bfb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_bidirectional_match(mouse_uri, predicted_human_uri, human_label):\n",
    "    \"\"\"Returns True if human→mouse returns the original mouse_uri as top match\"\"\"\n",
    "    # Use FAISS over mouse concepts to match human_label back\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    emb = model.encode([human_label])\n",
    "    emb = normalize_embeddings(np.array(emb).astype(np.float32))\n",
    "\n",
    "    D, I = mouse_index.search(emb, 1)\n",
    "    top_idx = I[0][0]\n",
    "    if top_idx == -1:\n",
    "        return False\n",
    "    top_mouse_uri = mouse_id_map.get(top_idx, {}).get(\"uri\")\n",
    "    return top_mouse_uri == mouse_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b60f2230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 73.57%\n",
      "Recall:    94.36%\n",
      "F1 Score:  82.68%\n",
      "Total evaluated: 1497\n",
      "Total skipped by threshold: 63\n"
     ]
    }
   ],
   "source": [
    "# Load matcher results and testset (now that both files are re-uploaded)\n",
    "with open(\"./data/datasets/anatomy-dataset/mouse_to_human_matches.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    matches = json.load(f)\n",
    "\n",
    "with open(\"./data/datasets/anatomy-dataset/mouse_testset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    testset = json.load(f)\n",
    "\n",
    "THRESHOLD = 0.8\n",
    "TP, FP, FN = 0, 0, 0\n",
    "\n",
    "# Build a lookup from URI to gold URI\n",
    "gold_lookup = {entry[\"uri\"]: entry[\"gold_uri\"] for entry in testset}\n",
    "\n",
    "# Evaluate results\n",
    "results = []\n",
    "for m in matches:\n",
    "    mouse_uri = m[\"mouse_uri\"]\n",
    "    mouse_label = m[\"mouse_label\"]\n",
    "    gold_uri = gold_lookup.get(mouse_uri)\n",
    "    if not gold_uri:\n",
    "        continue\n",
    "    #top_k = m[\"top_k_matches\"]\n",
    "    reranked = rerank_by_label_similarity(mouse_label, m[\"top_k_matches\"], 0.8, 0.2)\n",
    "    top_match = reranked[0]\n",
    "\n",
    "    if top_match[\"combined_score\"] < THRESHOLD:\n",
    "        FN += 1  # prediction too weak\n",
    "        continue\n",
    "\n",
    "    predicted_uri = top_match[\"uri\"]\n",
    "    is_correct = (predicted_uri == gold_uri)\n",
    "\n",
    "    # if not is_bidirectional_match(mouse_uri, predicted_uri, top_match[\"label\"]):\n",
    "    #     FN += 1\n",
    "    #     continue\n",
    "\n",
    "    if is_correct:\n",
    "        TP += 1\n",
    "    else:\n",
    "        FP += 1\n",
    "\n",
    "    found_at = next((i for i, match in enumerate(reranked) if match[\"uri\"] == gold_uri), None)\n",
    "\n",
    "    results.append({\n",
    "        \"mouse_uri\": mouse_uri,\n",
    "        \"gold_uri\": gold_uri,\n",
    "        \"predicted_uri\": reranked[0][\"uri\"] if reranked else None,\n",
    "        \"confidence\": reranked[0][\"score\"] if reranked else 0,\n",
    "        \"found_at_rank\": found_at\n",
    "    })\n",
    "\n",
    "# Compute metrics\n",
    "# total = len(results)\n",
    "# at_1 = sum(1 for r in results if r[\"found_at_rank\"] == 0) / total\n",
    "# at_5 = sum(1 for r in results if r[\"found_at_rank\"] is not None) / total\n",
    "\n",
    "# at_1, at_5, total\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall    = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1        = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {precision:.2%}\")\n",
    "print(f\"Recall:    {recall:.2%}\")\n",
    "print(f\"F1 Score:  {f1:.2%}\")\n",
    "print(f\"Total evaluated: {TP + FP + FN}\")\n",
    "print(f\"Total skipped by threshold: {FN}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckanenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
