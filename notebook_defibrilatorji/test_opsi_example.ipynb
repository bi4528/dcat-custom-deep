{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e23c27",
   "metadata": {},
   "source": [
    "# OPSI Ontology Matching Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdfb07",
   "metadata": {},
   "source": [
    "## Setup & Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046417c3",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789e3d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/hpc/home/bi4528/ckanenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, URIRef, RDFS, RDF, OWL, Literal, Namespace\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import yaml\n",
    "from typing import List, Dict, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rapidfuzz import fuzz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4a1b26",
   "metadata": {},
   "source": [
    "#### Load paths and constans from configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9118162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH = \"./notebook/source/pn2024_ontology_source.json\"\n",
    "SOURCE_INDEX_PATH = \"./notebook/source/source_index.faiss\"\n",
    "SOURCE_ID_TRACKER_PATH = \"./notebook/source/source_tracker.faiss\"\n",
    "TARGET_PATH = \"./notebook/target/preglednica_zahtevkov_zavarovanj_target.json\"\n",
    "TARGET_INDEX_PATH = \"./notebook/target/target_index.faiss\"\n",
    "TARGET_ID_TRACKER_PATH = \"./notebook/target/target_tracker.faiss\"\n",
    "OUTPUT_PATH = \"./notebook/result/csvw_alignment_results.json\"\n",
    "EMBEDDING_MODEL_NAME = \"intfloat/multilingual-e5-large\"\n",
    "TOP_K = 3\n",
    "SIMILARITY_THRESHOLD = 0.6\n",
    "HCB_ENABLED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d533de",
   "metadata": {},
   "source": [
    "#### Utils functions for ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e52721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b591d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(path):\n",
    "    graph = Graph()\n",
    "    graph.parse(path)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b9a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(graph, uri):\n",
    "    label = graph.value(uri, RDFS.label)\n",
    "    return str(label) if isinstance(label, Literal) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a86a2dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_related_uris(graph, subject, predicate):\n",
    "    \"\"\"Dereferences URIs linked by the predicate and returns their rdfs:label.\"\"\"\n",
    "    values = []\n",
    "    for obj in graph.objects(subject, predicate):\n",
    "        label = get_label(graph, obj)\n",
    "        if label:\n",
    "            values.append(label)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93f7df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_superclass_labels(graph, subject):\n",
    "    \"\"\"Get human-readable labels of direct superclasses.\"\"\"\n",
    "    super_labels = []\n",
    "    for superclass in graph.objects(subject, RDFS.subClassOf):\n",
    "        if isinstance(superclass, URIRef):\n",
    "            label = get_label(graph, superclass)\n",
    "            if label:\n",
    "                super_labels.append(label)\n",
    "        elif (superclass, RDF.type, OWL.Restriction) in graph:\n",
    "            filler = graph.value(superclass, OWL.someValuesFrom)\n",
    "            if isinstance(filler, URIRef):\n",
    "                super_labels.append(str(filler).split(\"#\")[-1])\n",
    "    return super_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fffb06",
   "metadata": {},
   "source": [
    "#### Utils functions for generating terms JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a4491df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_for_embedding(label, definition=None, synonyms=None, superclasses=None):\n",
    "    parts = [f\"Concept: {label}\"]\n",
    "\n",
    "    if synonyms:\n",
    "        parts.append(f\"Also known as: {', '.join(synonyms)}\")\n",
    "\n",
    "    if superclasses:\n",
    "        parts.append(f\"Part of: {', '.join(superclasses)}\")\n",
    "\n",
    "    if definition:\n",
    "        parts.append(f\"Defined as: {definition}\")\n",
    "\n",
    "    return \". \".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a3542f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_enriched_terms(graph):\n",
    "    terms = []\n",
    "    for s in graph.subjects(RDF.type, OWL.Class):\n",
    "        label = get_label(graph, s)\n",
    "        if not label:\n",
    "            continue\n",
    "\n",
    "        definition = extract_related_uris(graph, s, OBO.hasDefinition)\n",
    "        synonyms = extract_related_uris(graph, s, OBO.hasRelatedSynonym)\n",
    "        superclasses = extract_superclass_labels(graph, s)\n",
    "\n",
    "        enriched_text = build_text_for_embedding(\n",
    "            label=label,\n",
    "            definition=definition[0] if definition else None,\n",
    "            synonyms=synonyms,\n",
    "            superclasses=superclasses\n",
    "        )\n",
    "\n",
    "        terms.append({\n",
    "            \"uri\": str(s),\n",
    "            \"label\": label,\n",
    "            \"definition\": definition[0] if definition else \"\",\n",
    "            \"synonyms\": synonyms,\n",
    "            \"superclasses\": superclasses,\n",
    "            \"text_for_embedding\": enriched_text\n",
    "        })\n",
    "\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8cb1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
    "    return embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d9c7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ontology_indexing(ontology_json, faiss_index_path, id_tracker_json):\n",
    "\n",
    "    with open(ontology_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            ontology_terms = json.load(f)\n",
    "\n",
    "    texts = []\n",
    "    ids = []\n",
    "    valid_terms = []\n",
    "\n",
    "    for i, term in enumerate(ontology_terms):\n",
    "        text = term.get(\"text_for_embedding\")\n",
    "        if not text:\n",
    "            raise ValueError(\"Missing 'text_for_embedding'\")\n",
    "        texts.append(text)\n",
    "        ids.append(abs(hash(term[\"uri\"])) % (10**12))\n",
    "        valid_terms.append(term)\n",
    "\n",
    "    # Embedding\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    embeddings = model.encode(texts, batch_size=16, show_progress_bar=True)\n",
    "    embeddings = normalize_embeddings(np.array(embeddings))\n",
    "\n",
    "    # FAISS indexing\n",
    "    dimension = embeddings.shape[1]\n",
    "    base_index = faiss.IndexFlatIP(dimension)\n",
    "    index = faiss.IndexIDMap(base_index)\n",
    "    index.add_with_ids(embeddings, np.array(ids))\n",
    "    os.makedirs(os.path.dirname(faiss_index_path), exist_ok=True)\n",
    "    faiss.write_index(index, faiss_index_path)\n",
    "\n",
    "    # Save ID tracker\n",
    "    id_map = {str(id_): term for id_, term in zip(ids, valid_terms)}\n",
    "    with open(id_tracker_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(id_map, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b5389",
   "metadata": {},
   "source": [
    "#### Utils for matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d55dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_by_label_similarity(source_label: str, candidates: List[Dict], weight_faiss: float = 0.7, weight_label: float = 0.3) -> List[Dict]:\n",
    "    \"\"\"Combines semantic score and lexical similarity to rerank matches.\"\"\"\n",
    "    reranked = []\n",
    "    for match in candidates:\n",
    "        label_sim = fuzz.ratio(source_label, match[\"label\"]) / 100\n",
    "        combined_score = weight_faiss * match[\"score\"] + weight_label * label_sim\n",
    "        reranked.append({**match, \"combined_score\": combined_score})\n",
    "    return sorted(reranked, key=lambda x: x[\"combined_score\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23ca8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index_lookup(index_path: str, id_tracker_path: str):\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(id_tracker_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        id_map = {int(k): v for k, v in json.load(f).items()}\n",
    "    return index, id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "727dc5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faiss_batch_search(embeddings: np.ndarray, index, top_k: int):\n",
    "    return index.search(embeddings, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54b1ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_reverse_matches(\n",
    "    target_terms: List[Dict],\n",
    "    reverse_index,\n",
    "    reverse_id_map: Dict[int, Dict],\n",
    "    model,\n",
    "    top_k: int = 1\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Computes best reverse matches (target → source).\n",
    "    Returns dict: target_uri → best source_uri.\n",
    "    \"\"\"\n",
    "    reverse_lookup = {}\n",
    "\n",
    "    valid_terms = [t for t in target_terms if t.get(\"text_for_embedding\")]\n",
    "    texts = [t[\"text_for_embedding\"] for t in valid_terms]\n",
    "    uris = [t[\"uri\"] for t in valid_terms]\n",
    "\n",
    "    # Batch encoding\n",
    "    embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "    embeddings = normalize_embeddings(np.array(embeddings).astype(np.float32))\n",
    "\n",
    "    # Batch FAISS search\n",
    "    D, I = reverse_index.search(embeddings, top_k)\n",
    "\n",
    "    for i, (indices, scores) in enumerate(zip(I, D)):\n",
    "        best_idx = indices[0]\n",
    "        if best_idx == -1:\n",
    "            continue\n",
    "        match = reverse_id_map.get(best_idx)\n",
    "        if match:\n",
    "            reverse_lookup[uris[i]] = match[\"uri\"]\n",
    "\n",
    "    return reverse_lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "329a0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_reverse_matches_topk(\n",
    "    target_terms: list,\n",
    "    reverse_index,\n",
    "    reverse_id_map: dict,\n",
    "    model,\n",
    "    top_k: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Computes top-k reverse matches (target → source).\n",
    "    Returns dict: target_uri → list of source_uris.\n",
    "    \"\"\"\n",
    "    reverse_lookup_k = {}\n",
    "\n",
    "    valid_terms = [t for t in target_terms if t.get(\"text_for_embedding\")]\n",
    "    texts = [t[\"text_for_embedding\"] for t in valid_terms]\n",
    "    uris = [t[\"uri\"] for t in valid_terms]\n",
    "\n",
    "    # Batch encoding\n",
    "    embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    # Batch FAISS search\n",
    "    D, I = reverse_index.search(np.array(embeddings).astype(np.float32), top_k)\n",
    "\n",
    "    for i, indices in enumerate(I):\n",
    "        matches = [reverse_id_map[idx][\"uri\"] for idx in indices if idx != -1 and reverse_id_map.get(idx)]\n",
    "        reverse_lookup_k[uris[i]] = matches\n",
    "\n",
    "    return reverse_lookup_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_terms(terms: List[Dict], model) -> np.ndarray:\n",
    "    texts = [t[\"text_for_embedding\"] for t in terms if t.get(\"text_for_embedding\")]\n",
    "    return normalize_embeddings(model.encode(texts, batch_size=32, show_progress_bar=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06e062ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_faiss_results(source_terms, D, I, target_id_map):\n",
    "    matches = []\n",
    "    for i, (distances, indices) in enumerate(zip(D, I)):\n",
    "        src = source_terms[i]\n",
    "        results = []\n",
    "        for idx, score in zip(indices, distances):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            t = target_id_map.get(idx)\n",
    "            if not t:\n",
    "                continue\n",
    "            results.append({\n",
    "                \"uri\": t[\"uri\"],\n",
    "                \"label\": t[\"label\"],\n",
    "                \"score\": float(score)\n",
    "            })\n",
    "        matches.append({\n",
    "            \"source_uri\": src[\"uri\"],\n",
    "            \"source_label\": src[\"label\"],\n",
    "            \"top_k_matches\": results\n",
    "        })\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c54f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_label_reranking(matches: List[Dict]):\n",
    "    reranked = []\n",
    "    for match in matches:\n",
    "        ranked = rerank_by_label_similarity(match[\"source_label\"], match[\"top_k_matches\"])\n",
    "        match[\"top_k_matches\"] = ranked\n",
    "        match[\"top_match\"] = ranked[0] if ranked else None\n",
    "        reranked.append(match)\n",
    "    return reranked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ffeab",
   "metadata": {},
   "source": [
    "#### Loading RDF into graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b776c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_graph = load_json(SOURCE_PATH)\n",
    "target_graph = load_json(TARGET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf48b3",
   "metadata": {},
   "source": [
    "#### Loading embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0e6e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee7dabe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uri': 'n16ce517f066c4ab4961e42cfc37c28b2b2',\n",
       "  'label': 'datum škodnega dogodka',\n",
       "  'definition': 'Datum, ko je bila škoda prijavljena zavarovalnici.',\n",
       "  'synonyms': ['datumPN'],\n",
       "  'superclasses': [],\n",
       "  'text_for_embedding': 'Concept: datum škodnega dogodka. Also known as: datumPN. Defined as: Datum, ko je bila škoda prijavljena zavarovalnici.'},\n",
       " {'uri': 'n16ce517f066c4ab4961e42cfc37c28b2b3',\n",
       "  'label': 'vzrok nastanka škode',\n",
       "  'definition': 'Opis ali klasifikacija primarnega vzroka škode.',\n",
       "  'synonyms': [],\n",
       "  'superclasses': [],\n",
       "  'text_for_embedding': 'Concept: vzrok nastanka škode. Defined as: Opis ali klasifikacija primarnega vzroka škode.'},\n",
       " {'uri': 'n16ce517f066c4ab4961e42cfc37c28b2b4',\n",
       "  'label': 'kraj dogodka',\n",
       "  'definition': 'Ime kraja ali regije, kjer se je zgodil škodni primer.',\n",
       "  'synonyms': [],\n",
       "  'superclasses': [],\n",
       "  'text_for_embedding': 'Concept: kraj dogodka. Defined as: Ime kraja ali regije, kjer se je zgodil škodni primer.'},\n",
       " {'uri': 'n16ce517f066c4ab4961e42cfc37c28b2b5',\n",
       "  'label': 'vrsta škode',\n",
       "  'definition': 'Materialna ali telesna škoda, lahko tudi kombinacija.',\n",
       "  'synonyms': [],\n",
       "  'superclasses': [],\n",
       "  'text_for_embedding': 'Concept: vrsta škode. Defined as: Materialna ali telesna škoda, lahko tudi kombinacija.'},\n",
       " {'uri': 'n16ce517f066c4ab4961e42cfc37c28b2b6',\n",
       "  'label': 'starost prijavitelja',\n",
       "  'definition': 'Starost osebe, ki je prijavila škodo, v letih.',\n",
       "  'synonyms': [],\n",
       "  'superclasses': [],\n",
       "  'text_for_embedding': 'Concept: starost prijavitelja. Defined as: Starost osebe, ki je prijavila škodo, v letih.'},\n",
       " {'uri': 'n16ce517f066c4ab4961e42cfc37c28b2b7',\n",
       "  'label': 'spol osebe',\n",
       "  'definition': 'Spol zavarovanca ali prijavitelja dogodka.',\n",
       "  'synonyms': [],\n",
       "  'superclasses': [],\n",
       "  'text_for_embedding': 'Concept: spol osebe. Defined as: Spol zavarovanca ali prijavitelja dogodka.'},\n",
       " {'uri': 'n16ce517f066c4ab4961e42cfc37c28b2b8',\n",
       "  'label': 'X koordinata',\n",
       "  'definition': 'Geografska širina dogodka v državnem koordinatnem sistemu.',\n",
       "  'synonyms': [],\n",
       "  'superclasses': [],\n",
       "  'text_for_embedding': 'Concept: X koordinata. Defined as: Geografska širina dogodka v državnem koordinatnem sistemu.'},\n",
       " {'uri': 'n16ce517f066c4ab4961e42cfc37c28b2b9',\n",
       "  'label': 'Y koordinata',\n",
       "  'definition': 'Geografska dolžina dogodka v državnem koordinatnem sistemu.',\n",
       "  'synonyms': [],\n",
       "  'superclasses': [],\n",
       "  'text_for_embedding': 'Concept: Y koordinata. Defined as: Geografska dolžina dogodka v državnem koordinatnem sistemu.'},\n",
       " {'uri': 'n16ce517f066c4ab4961e42cfc37c28b2b10',\n",
       "  'label': 'rezultat alkotesta',\n",
       "  'definition': 'Izmerjena vrednost alkohola pri osebi, v g/L.',\n",
       "  'synonyms': [],\n",
       "  'superclasses': [],\n",
       "  'text_for_embedding': 'Concept: rezultat alkotesta. Defined as: Izmerjena vrednost alkohola pri osebi, v g/L.'},\n",
       " {'uri': 'n16ce517f066c4ab4961e42cfc37c28b2b11',\n",
       "  'label': 'vozniski staž v letih',\n",
       "  'definition': 'Vozniške izkušnje prijavitelja v letih.',\n",
       "  'synonyms': [],\n",
       "  'superclasses': [],\n",
       "  'text_for_embedding': 'Concept: vozniski staž v letih. Defined as: Vozniške izkušnje prijavitelja v letih.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038506e9",
   "metadata": {},
   "source": [
    "#### Generate enriched JSON of ontology terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d7e9de",
   "metadata": {},
   "source": [
    "Generate enriched terms for human ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9379a4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'subjects'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m target_terms = \u001b[43mextract_enriched_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(TARGET_PATH, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m     json.dump(target_terms, f, indent=\u001b[32m2\u001b[39m, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mextract_enriched_terms\u001b[39m\u001b[34m(graph)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_enriched_terms\u001b[39m(graph):\n\u001b[32m      2\u001b[39m     terms = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubjects\u001b[49m(RDF.type, OWL.Class):\n\u001b[32m      4\u001b[39m         label = get_label(graph, s)\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m label:\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'subjects'"
     ]
    }
   ],
   "source": [
    "target_terms = extract_enriched_terms(target_graph)\n",
    "\n",
    "with open(TARGET_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(target_terms, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed3f5f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_terms = target_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e4d5ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd135321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b79e09ec",
   "metadata": {},
   "source": [
    "Generate enriched terms for mouse ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_terms = extract_enriched_terms(source_graph)\n",
    "\n",
    "with open(SOURCE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(source_terms, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5658b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_terms = source_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2e313ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7327aca2",
   "metadata": {},
   "source": [
    "#### Ontology indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68942e1f",
   "metadata": {},
   "source": [
    "Create and populate FAISS with human entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c1e2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:10<00:00, 10.32s/it]\n"
     ]
    }
   ],
   "source": [
    "ontology_indexing(ontology_json=TARGET_PATH, faiss_index_path=TARGET_INDEX_PATH, id_tracker_json=TARGET_ID_TRACKER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5a39c",
   "metadata": {},
   "source": [
    "Create and populate FAISS with mouse entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57e43803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:02<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "ontology_indexing(ontology_json=SOURCE_PATH, faiss_index_path=SOURCE_INDEX_PATH, id_tracker_json=SOURCE_ID_TRACKER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e0478",
   "metadata": {},
   "source": [
    "#### Execute matching on mouse human pair of ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed62e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_index = faiss.read_index(TARGET_INDEX_PATH)\n",
    "with open(TARGET_ID_TRACKER_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    target_id_map = {int(k): v for k, v in json.load(f).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d373bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_index = faiss.read_index(SOURCE_INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b705e5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 13.70it/s]\n"
     ]
    }
   ],
   "source": [
    "embs = embed_terms(source_terms, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7f0a5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.03it/s]\n"
     ]
    }
   ],
   "source": [
    "embs_h = embed_terms(target_terms, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "884de1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15effa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len match: 35\n",
      "len match: 35\n"
     ]
    }
   ],
   "source": [
    "# Step-by-step matching\n",
    "D, I = faiss_batch_search(embs, target_index, top_k=5)\n",
    "matches = map_faiss_results(source_terms, D, I, target_id_map)\n",
    "\n",
    "output_path = \"./notebook/result/mouse_to_human_matches_unranked.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(matches, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"len match: {len(matches)}\")\n",
    "\n",
    "# Optional refinements\n",
    "matches = apply_label_reranking(matches)\n",
    "\n",
    "output_path = \"./notebook/result/mouse_to_human_matches_ranked.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(matches, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"len match: {len(matches)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98265564",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_index = faiss.read_index(SOURCE_INDEX_PATH)\n",
    "with open(SOURCE_ID_TRACKER_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    mouse_id_map = {int(k): v for k, v in json.load(f).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfba7be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.12it/s]\n"
     ]
    }
   ],
   "source": [
    "reverse_lookup = precompute_reverse_matches(\n",
    "    target_terms=target_terms,\n",
    "    reverse_index=mouse_index,\n",
    "    reverse_id_map=mouse_id_map,\n",
    "    model=embedding_model,\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4027746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 40.79it/s]\n"
     ]
    }
   ],
   "source": [
    "reverse_lookup_topk = precompute_reverse_matches_topk(\n",
    "    target_terms=target_terms,\n",
    "    reverse_index=mouse_index,\n",
    "    reverse_id_map=mouse_id_map,\n",
    "    model=embedding_model,\n",
    "    top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "962260bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reverse_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39c1f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"./notebook/result/reverse_lookup.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reverse_lookup, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abcf5460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hcb(matches, reverse_lookup, fallback_threshold=0.95):\n",
    "    filtered = []\n",
    "    skipped_top_match = 0\n",
    "    no_predicited_uri = 0\n",
    "    for m in matches:\n",
    "        source_uri = m[\"source_uri\"]\n",
    "        top_match = m.get(\"top_match\")\n",
    "\n",
    "        if not top_match:\n",
    "            skipped_top_match += 1\n",
    "            continue\n",
    "\n",
    "        predicted_uri = top_match[\"uri\"]\n",
    "        confidence = top_match.get(\"combined_score\", top_match.get(\"score\", 0))\n",
    "\n",
    "        if reverse_lookup.get(predicted_uri) == source_uri:\n",
    "            filtered.append(m)  # standard HCB\n",
    "        elif confidence >= fallback_threshold:\n",
    "            filtered.append(m)  # allow fallback based on confidence\n",
    "        else: \n",
    "            no_predicited_uri += 1\n",
    "\n",
    "    print(f\"skipped_top_match {skipped_top_match}\")\n",
    "    print(f\"no_predicited_uri {no_predicited_uri}\")\n",
    "\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78e88f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hcb_with_topk(matches, reverse_lookup_top1, reverse_lookup_topk):\n",
    "    \"\"\"\n",
    "    Filters matches using bidirectional match (HCB), extended to top-k reverse lookup and confidence fallback.\n",
    "    \n",
    "    Args:\n",
    "        matches: list of match dicts (with top_match + score)\n",
    "        reverse_lookup_top1: dict[target_uri → best source_uri]\n",
    "        reverse_lookup_topk: dict[target_uri → list of top-k source_uris]\n",
    "        fallback_threshold: minimum score to allow fallback if HCB fails\n",
    "\n",
    "    Returns:\n",
    "        List of filtered matches\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    stats = {\n",
    "        \"strict_hcb\": 0,\n",
    "        \"semi_hcb\": 0,\n",
    "        \"skipped\": 0\n",
    "    }\n",
    "\n",
    "    for m in matches:\n",
    "        source_uri = m[\"source_uri\"]\n",
    "        top_match = m.get(\"top_match\")\n",
    "\n",
    "        if not top_match:\n",
    "            stats[\"skipped\"] += 1\n",
    "            continue\n",
    "\n",
    "        predicted_uri = top_match[\"uri\"]\n",
    "\n",
    "        if reverse_lookup_top1.get(predicted_uri) == source_uri:\n",
    "            filtered.append(m)\n",
    "            stats[\"strict_hcb\"] += 1\n",
    "        elif source_uri in reverse_lookup_topk.get(predicted_uri, []):\n",
    "            filtered.append(m)\n",
    "            stats[\"semi_hcb\"] += 1\n",
    "        else:\n",
    "            stats[\"skipped\"] += 1\n",
    "\n",
    "    print(\"HCB Filtering Summary:\")\n",
    "    for key, count in stats.items():\n",
    "        print(f\"  {key}: {count}\")\n",
    "\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2457c579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCB Filtering Summary:\n",
      "  strict_hcb: 10\n",
      "  semi_hcb: 2\n",
      "  skipped: 23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = apply_hcb_with_topk(matches, reverse_lookup, reverse_lookup_topk)\n",
    "\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bca9ed0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching complete. Results saved to ./notebook/result/opsi_match.json\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./notebook/result/opsi_match.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(matches, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Matching complete. Results saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckanenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
